{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trees_ensambling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2020/blob/master/04_lab/trees_ensambling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-MpKxz14SSU"
      },
      "source": [
        "# Decision trees, ensembling and model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KWa4yz4qbxkF"
      },
      "source": [
        "![img](https://pbs.twimg.com/media/B13n2VVCIAA0hJS.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7EBDdjJ-ay0M",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zguy6o5Pb3-x"
      },
      "source": [
        "Let's generate a toy dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdtf3-9WauYy",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_toy, y_toy = make_blobs(n_samples=400,\n",
        "                          centers=[[0., 1.], [1., 2.]],\n",
        "                          random_state=14)\n",
        "\n",
        "plt.scatter(X_toy[:, 0], X_toy[:, 1], c=y_toy, alpha=0.8, cmap='bwr')\n",
        "plt.xlabel('X1'), plt.ylabel('X2');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u9V31PoNcFuO"
      },
      "source": [
        "## Decision trees out of the box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBmN8Ildbe0d",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yr-JTRwCcDR1"
      },
      "source": [
        "DecisionTreeClassifier has a number of parameters:\n",
        "* `max_depth` – a limit on tree depth (default – no limit)\n",
        "* `min_samples_split` – there should be at least this many samples to split further (default – 2)\n",
        "* `min_samples_leaf` – there should be at least this many samples on one side of a split to consider it valid (default – 1).\n",
        "* `criterion` – 'gini' or 'entropy' – split stuff over this parameter (default : gini)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KX2Dm7onbq2s",
        "colab": {}
      },
      "source": [
        "clf = DecisionTreeClassifier(min_samples_leaf=15)\n",
        "clf.fit(X_toy, y_toy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IM02fOjxcMic"
      },
      "source": [
        "### Plot decision surface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBVDpZ6Y-jB5",
        "colab_type": "text"
      },
      "source": [
        "Here's a function that makes a 2d decision boundary plot for a given classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XGp3PLfW35hF",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def plot_decision_surface(\n",
        "                  clf, X, y,\n",
        "                  grid_step=0.02,\n",
        "                  cmap='bwr',\n",
        "                  alpha=0.6,\n",
        "        ):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary of clf on X and y, visualize training points\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the grid\n",
        "    x_top_left = X.min(axis=0) - 1\n",
        "    x_bottom_right = X.max(axis=0) + 1\n",
        "    grid_x0, grid_x1 = np.meshgrid(\n",
        "         np.arange(x_top_left[0], x_bottom_right[0], grid_step),\n",
        "         np.arange(x_top_left[1], x_bottom_right[1], grid_step)\n",
        "      )\n",
        "    \n",
        "    # Calculate predictions on the grid\n",
        "    y_pred_grid = clf.predict(\n",
        "                        np.stack(\n",
        "                              [\n",
        "                                grid_x0.ravel(),\n",
        "                                grid_x1.ravel()\n",
        "                              ],\n",
        "                              axis=1\n",
        "                            )\n",
        "                      ).reshape(grid_x1.shape)\n",
        "    \n",
        "    # Find optimal contour levels and make a filled\n",
        "    # contour plot of predictions\n",
        "    labels = np.sort(np.unique(y))\n",
        "    labels = np.concatenate([[labels[0] - 1],\n",
        "                             labels,\n",
        "                             [labels[-1] + 1]])\n",
        "    medians = (labels[1:] + labels[:-1]) / 2\n",
        "    plt.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
        "                 levels=medians)\n",
        "    \n",
        "    # Scatter data points on top of the plot,\n",
        "    # with different styles for correct and wrong\n",
        "    # predictions\n",
        "    y_pred = clf.predict(X)\n",
        "    plt.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
        "                marker='o', cmap=cmap, s=10, label='correct')\n",
        "    plt.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
        "                marker='x', cmap=cmap, s=50, label='errors')\n",
        "\n",
        "    # Dummy plot call to print the accuracy in the legend.\n",
        "    plt.plot([], [], ' ',\n",
        "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
        "    \n",
        "    plt.legend(loc='best')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ODNVnC-jB9",
        "colab_type": "text"
      },
      "source": [
        "Let's apply it to the tree we've fitted above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_2BZxCrb0tn",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plot_decision_surface(clf, X_toy, y_toy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7RWucAGNcY-t"
      },
      "source": [
        "### Tree depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImiOildL-jCE",
        "colab_type": "text"
      },
      "source": [
        "First we are going to split our data to train and test subsets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6clG_qq-jCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_toy_train, X_toy_test, y_toy_train, y_toy_test = \\\n",
        "    train_test_split(X_toy, y_toy, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeL0PME0-jCI",
        "colab_type": "text"
      },
      "source": [
        "Now it's your turn to investigate how the decision boundary depends on the tree depth. Maximum tree depth is defined by the `max_depth` parameter. Try out the following values: `[1, 2, 3, 5, 10]`. Make decision boundary plots for both train and test datasets (separately).\n",
        "\n",
        "  > *Hint: you can make a nice plot with multiple columns and rows (see example below).*\n",
        "  \n",
        "```python\n",
        "plt.figure(figsize=(width, height))\n",
        "for i in range(num_rows * num_columns):\n",
        "  plt.subplot(num_rows, num_columns, i + 1)\n",
        "  # subplot numbering starts from 1   ^^^\n",
        "  \n",
        "  # ...\n",
        "  # (do the plotting)\n",
        "plt.show();\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppdUZd6U-jCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "depth_values = [1, 2, 3, 5, 10]\n",
        "\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5lgCWhbEc77C"
      },
      "source": [
        "### Toy multiclass data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48yemIKk-jCQ",
        "colab_type": "text"
      },
      "source": [
        "Now let's try out a multiclass classification case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wahVk0jegnb_",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/yandexdataschool/MLatImperial2020/raw/master/04_lab/data.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7iUjzyo-jCU",
        "colab_type": "text"
      },
      "source": [
        "Firstly, we'll load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2qKojahYhm1",
        "colab": {}
      },
      "source": [
        "data = np.load('data.npz')\n",
        "X, y = data[\"X\"], data[\"y\"]\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cuuN5L6-jCb",
        "colab_type": "text"
      },
      "source": [
        "And then split it to train and test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ybPi8TNtYlgV",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=0.5, random_state=1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYgX24sd-jCh",
        "colab_type": "text"
      },
      "source": [
        "Now it's your turn to have a look at the data. Make a 2d scatter plot of the data points.\n",
        "\n",
        " > *Hint: instead of calling `scatter` separately for each class, you can give it a vector of color index values through the `c` parameter (`scatter(x0, x1, c=y, cmap='rainbow'`). The 'rainbow' colormap gives good enough color diversity for the multiclass case.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5l3oJDs-jCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "<YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6NA1BGH-jCo",
        "colab_type": "text"
      },
      "source": [
        "Now that we've had a look at the data, let's fit a decision tree on it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-dvkVwtLZjDA",
        "colab": {}
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3s03yi0-jCs",
        "colab_type": "text"
      },
      "source": [
        "and plot the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XEHFL51JY02v",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 16))\n",
        "plt.subplot(2, 1, 1)\n",
        "plot_decision_surface(clf, X_train, y_train, cmap='rainbow', grid_step=0.2)\n",
        "plt.subplot(2, 1, 2)\n",
        "plot_decision_surface(clf, X_test, y_test, cmap='rainbow', grid_step=0.2);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KtKtjwBDdKZA"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jmEoJSDxdHHs"
      },
      "source": [
        "#### We need a better tree!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VwZ4BVxobkiu"
      },
      "source": [
        "Try adjusting the parameters of DecisionTreeClassifier to improve the test accuracy.\n",
        " * Accuracy >= 0.72 - not bad for a start\n",
        " * Accuracy >= 0.75 - better, but not enough\n",
        " * Accuracy >= 0.77 - pretty good\n",
        " * Accuracy >= 0.78 - great! (probably the best result for a single tree)\n",
        " \n",
        "Feel free to modify the DecisionTreeClassifier above instead of re-writing everything.\n",
        "\n",
        "**Note:** some of the parameters you can tune are under the \"Decision trees out of the box\" header."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm8lSZx-K5C",
        "colab_type": "text"
      },
      "source": [
        "#### Bonus quest\n",
        "Try adding feature transformations using a pipeline and a function transformer, e.g.:\n",
        "```python\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "clf = make_pipeline(\n",
        "    FunctionTransformer(lambda X: np.concatenate([X, X**2], axis=1)),\n",
        "    DecisionTreeClassifier()\n",
        ")\n",
        "```\n",
        "\n",
        "Which transformations should improve the score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e0sTlV_msz3H"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvifS1edLmnd",
        "colab_type": "text"
      },
      "source": [
        "We've talked a lot about the importance of feature scaling. Why aren't we doing it here?\n",
        "\n",
        "Try adding a standard scaler to the pipeline of your best model and check how it affects the result. Can you explain the result?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX25hYFvL8DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHT8J3bfLlWM",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UmU84QKFs1qx"
      },
      "source": [
        "## Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8S1A9-L4tAjo"
      },
      "source": [
        "Let's build our own decision tree bagging and see how well it works. Implement the **`BagOfTrees`** class below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eQhRF4kChB18",
        "colab": {}
      },
      "source": [
        "class BagOfTrees:\n",
        "  def __init__(self, n_estimators=10, **kwargs):\n",
        "    self.trees = []\n",
        "    for i in range(n_estimators):\n",
        "        self.trees.append(DecisionTreeClassifier(**kwargs))\n",
        "        \n",
        "  def fit(self, X, y):\n",
        "    trees = self.trees\n",
        "    \n",
        "    # Fit each of the trees on a random subset of X and y.\n",
        "    # hint: you can select random subsample of data like this:\n",
        "    # >>> ix = np.random.randint(0, len(X), len(X))\n",
        "    # >>> X_sample, y_sample = X[ix], y[ix]\n",
        "\n",
        "    <YOUR CODE>\n",
        "    \n",
        "  def predict(self, X):\n",
        "    trees = self.trees\n",
        "    \n",
        "    # Compute predictions of each tree and aggregate them into\n",
        "    # the ensemble prediction\n",
        "    # Note: you can use tree.predict(X) to get the predicted classes\n",
        "    # or tree.predict_proba(X) to get individual probabilities\n",
        "    # for all classes\n",
        "    \n",
        "    return <YOURCODE - np.array of prediction indices>\n",
        "  \n",
        "# once you think you're done, see if your code passes the asserts below"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KMwTGrE6xQxY",
        "colab": {}
      },
      "source": [
        "model = BagOfTrees(n_estimators=100, min_samples_leaf=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test[::100])\n",
        "print(\"predictions:\", pred)\n",
        "assert isinstance(pred, np.ndarray), \"prediction must be a numpy array\"\n",
        "assert str(pred.dtype).startswith('int'), \"prediction dtype must be integer (int32/int64)\"\n",
        "assert pred.ndim == 1, \"prediction must be a vector (1-dimensional)\"\n",
        "assert len(pred) == len(X_test[::100]), \"must predict exactly one answer for each input (expected length %i, got %i)\" % (len(X_test[::100]), len(pred))\n",
        "assert any(model.trees[0].predict(X_train) != model.trees[1].predict(X_train)), \"Some trees are identical. Did you forget to train each tree on a random part of the data?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqkgnB_-jDH",
        "colab_type": "text"
      },
      "source": [
        "If the cell above executes without errors, run the code below to compare overall accuracy with individual tree accuracies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WuVXG232hBuy",
        "colab": {}
      },
      "source": [
        "for i, tree in enumerate(model.trees[:5]):\n",
        "    print(\"tree {} individual accuracy = {:.5f}\".format(\n",
        "        i, accuracy_score(y_test, tree.predict(X_test))\n",
        "      ))\n",
        "\n",
        "print(\"Ensemble accuracy:\",\n",
        "      accuracy_score(model.predict(X_test), y_test)) # should be >= 0.79"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh5htnXF-jDM",
        "colab_type": "text"
      },
      "source": [
        "And have a look at the decision surface:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1X3TZ1iMx3oO",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 16))\n",
        "plt.subplot(2, 1, 1)\n",
        "plot_decision_surface(model, X_train, y_train, cmap='rainbow', grid_step=0.4)\n",
        "plt.subplot(2, 1, 2)\n",
        "plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.4);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3wX9mTNCWUn",
        "colab_type": "text"
      },
      "source": [
        "Now let's check how train and test accuracy depends on the number of estimators.\n",
        "\n",
        "Hint: instead of fitting a new BagOfTrees for each number of estimators we can just fit the maximum number and then iteratively predict and remove the fitted trees one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx5pt9QMDRyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BagOfTrees(n_estimators=100, min_samples_split=30, splitter='random', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "n_trees = []\n",
        "test_score = []\n",
        "train_score = []\n",
        "\n",
        "<YOUR CODE HERE> # fill the lists above to make the plot\n",
        "\n",
        "plt.plot(n_trees, train_score, label='train')\n",
        "plt.plot(n_trees, test_score, label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('number of trees')\n",
        "plt.ylabel('accuracy');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bkW93y5C0Mqw"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mzvrpP_0UnQ"
      },
      "source": [
        "### Pre-implemented ensembles: Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ECeEe3MH05BB"
      },
      "source": [
        "RandomForest combines bagging and random subspaces: each tree uses a fraction of training samples, and the splits are chosen among subsets of features. Typically this leads to a slightly better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGns4GcZx3kM",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Task: create and fit a random forest with\n",
        "# 100 estimators and at least 5 samples per leaf\n",
        "\n",
        "model = <YOUR CODE>\n",
        "\n",
        "<YOUR CODE>\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FfDOLLV22djx"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YtM1TZql2fCP"
      },
      "source": [
        "### Pre-implemented ensembles: Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXbycsrJYQTW",
        "colab_type": "text"
      },
      "source": [
        "One of the most commonly used libraries for gradient boosing is the [XGBoost library](https://xgboost.ai/). Consider reading [this document](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) for an introduction to the algorithm.\n",
        "\n",
        "Here's the [help page](https://xgboost.readthedocs.io/en/latest/parameter.html) listing available parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zamTOiyMZNHD",
        "colab_type": "text"
      },
      "source": [
        "Let's start by importing the classifier class and the function that plots individual trees as graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ocW-MLi90LxD",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier, plot_tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK3C_XPjZtfU",
        "colab_type": "text"
      },
      "source": [
        "We can now investigate how decision surface depends on the number of trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny94kTDqPzPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n_estimators in range(1,10):\n",
        "    model = <YOUR CODE> # create an XGBClassifier with trees of depth 1,\n",
        "                        # learning rate 0.5 and n_estimators estimators\n",
        "\n",
        "    <YOUR CODE> # fit this model to the train data\n",
        "\n",
        "    print(\"n_estimators = \", n_estimators)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.4)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paFmsQONaQfc",
        "colab_type": "text"
      },
      "source": [
        "And here's how one may use the `plot_tree` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPp5qkVnXJbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 9))\n",
        "plot_tree(model, num_trees=44, ax=ax, dpi='400');\n",
        "#                   ^^^ This parameter selects the\n",
        "#                       tree that you want to plot.\n",
        "#                       Since there's 9 estimators\n",
        "#                       in the last model and 5\n",
        "#                       classes in our data, the total\n",
        "#                       amount of individual trees\n",
        "#                       is 45 (from 0 to 44)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSu9dBVnvawb",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**Warning:**</font> current xgboost implementation is not very safe to typos, i.e. it can silently swallow whatever argument you provide, even if it has no effect, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoh-aaQlv5-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XGBClassifier(abrakadabra=\"I won't change anything\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq_GvLOgwMHW",
        "colab_type": "text"
      },
      "source": [
        "so be sure to check your spelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErviY92bwdBE",
        "colab_type": "text"
      },
      "source": [
        "Now let's try to improve the score by adjusting the parameters. Here are some of the parameters you may want to try:\n",
        "  - `max_depth` – maximum tree depth,\n",
        "  - `n_estimators` – number of trees (per class),\n",
        "  - `learning_rate` – shrinkage,\n",
        "  - `reg_lambda` – L2 regularization term on weights,\n",
        "  - `subsample` – row random subsampling rate (per tree),\n",
        "  - `colsample_bynode` – column subsampling rate (per node)\n",
        "  - `gamma` – minimum loss reduction required to make a further partition on a leaf node of the tree\n",
        "\n",
        "See [this page](https://xgboost.readthedocs.io/en/latest/parameter.html) for more information.\n",
        "\n",
        "  > *Hint: since XGBClassifier has the same interface as sklearn models, you can use GridSearchCV on it if you want.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTt9TgEIwjrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = <YOUR CODE HERE>\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_decision_surface(model, X_test, y_test, cmap='rainbow', grid_step=0.8)\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mBpPvhuS3Xa0"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WehLBvvh3gQb"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rPGAIyyF-Nli"
      },
      "source": [
        "For this example we are going to use California Housing Dataset.\n",
        "\n",
        "The **target** variable is the median house value for California districts.\n",
        "\n",
        "\n",
        "The features are:\n",
        " *   **MedInc** median income in block\n",
        " *   **HouseAge** median house age in block\n",
        " *   **AveRooms** average number of rooms\n",
        " *   **AveBedrms** average number of bedrooms\n",
        " *   **Population** block population\n",
        " *   **AveOccup** average house occupancy\n",
        " *   **Latitude** house block latitude\n",
        " *   **Longitude** house block longitude\n",
        " \n",
        " More information [here](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAIX2GlA3o1Z",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nVuypXO13vjy",
        "colab": {}
      },
      "source": [
        "dataset = fetch_california_housing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAKa7Uy-dnB",
        "colab_type": "text"
      },
      "source": [
        "`dataset` holds the data in numpy arrays, but we can convert it to a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wM50xJNq4gbE",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "data['target'] = dataset.target\n",
        "\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMCRUqM9-s_H",
        "colab_type": "text"
      },
      "source": [
        "Now, let's separate the features from the target and split the data to train and test parts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hoxs850jQuT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_X = <YOUR CODE>\n",
        "data_y = <YOUR CODE>\n",
        "\n",
        "X_train, X_test, y_train, y_test = <YOUR CODE> # make a 50:50 split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYurIqUG-4Fj",
        "colab_type": "text"
      },
      "source": [
        "and grid search for best random forest parameters on it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-UXdggjT7VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=30)\n",
        "\n",
        "gscv = GridSearchCV(model,\n",
        "                    param_grid=<YOUR CODE HERE>,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    n_jobs=-1,\n",
        "                    cv=3,\n",
        "                    verbose=3)\n",
        "\n",
        "gscv.fit(X_train, y_train)\n",
        "model = gscv.best_estimator_\n",
        "print(model)\n",
        "\n",
        "print(\"Train loss:\", mean_squared_error(y_train, model.predict(X_train)))\n",
        "print(\"Test loss:\" , mean_squared_error(y_test , model.predict(X_test )))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCyXJiJl-_Ny",
        "colab_type": "text"
      },
      "source": [
        "Once fit, the model has the information about importances of individual features, calculated from gain in individual splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SOGUyPDk7XU5",
        "colab": {}
      },
      "source": [
        "# get the estimates of feature importances\n",
        "importances = model.feature_importances_\n",
        "# calculate the std:\n",
        "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# This part will be used to make nice x-axis labels\n",
        "# (we'll tell matplotlib to convert numeric feature\n",
        "# index to a text label):\n",
        "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
        "def format_fn(tick_val, tick_pos):\n",
        "    if int(tick_val) in range(len(importances)):\n",
        "        return dataset.feature_names[int(tick_val)]\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(13,7))\n",
        "ax.set_title(\"Feature importances\")\n",
        "ax.xaxis.set_major_formatter(FuncFormatter(format_fn))\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "ax.bar(range(len(importances)), importances[indices],\n",
        "       color=\"r\", yerr=std[indices], align=\"center\")\n",
        "ax.set_xlim([-1, len(importances)])\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lhUHlwfkqI1i"
      },
      "source": [
        "## Bonus part: visualizing a tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tz9q62nkhBnU",
        "colab": {}
      },
      "source": [
        "import pydot_ng\n",
        "import io\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import SVG\n",
        "\n",
        "\n",
        "def plot_tree(tree, columns, max_depth):\n",
        "  buf = io.StringIO()\n",
        "  export_graphviz(tree, out_file=buf,\n",
        "                  feature_names=columns,\n",
        "                  max_depth=max_depth)\n",
        "  graph = pydot_ng.graph_from_dot_data(buf.getvalue())\n",
        "  return SVG(graph.create_svg())\n",
        "\n",
        "plot_tree(model.estimators_[0], dataset.feature_names, max_depth=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gc5u9kFx3Y9o"
      },
      "source": [
        "## Bonus part (2): Genetic hyperparameter search: TPOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8zt_kRpCrfF",
        "colab_type": "text"
      },
      "source": [
        "There's a nice tool called [TPOT](https://github.com/EpistasisLab/tpot) that applies genetic algorithms to search for an optimal pipeline and hyperparameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RvGc5pMq_sCb",
        "colab": {}
      },
      "source": [
        "!pip install tpot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYttFqQDGki",
        "colab_type": "text"
      },
      "source": [
        "Here's a brief example of using TPOT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FaKU94D6_xrE",
        "colab": {}
      },
      "source": [
        "# import the model\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "# create an instance\n",
        "tpot = TPOTRegressor(generations=2, population_size=5, verbosity=3)\n",
        "\n",
        "# run the search:\n",
        "tpot.fit(X_train.values, y_train.values)\n",
        "\n",
        "# print the scoe:\n",
        "print(tpot.score(X_test.values, y_test.values))\n",
        "\n",
        "# export the optimal pipeline as a python script\n",
        "tpot.export('tpot_california_pipeline.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQDxkomDnkf",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at the optimal model we've found:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IW1VQQx6AaIX",
        "colab": {}
      },
      "source": [
        "with open('tpot_california_pipeline.py') as f:\n",
        "  print(''.join(f.readlines()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0BBtw2VjnF",
        "colab_type": "text"
      },
      "source": [
        "There's many other hyperparameter optimization methods, many implemented in the [scikit-optimize](https://scikit-optimize.github.io/) package, e.g. bayessian optimization with gaussian process ([see this link](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html) for a nice tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "de_TKyP7JfZ2"
      },
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```"
      ]
    }
  ]
}
